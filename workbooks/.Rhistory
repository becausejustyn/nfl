title = "BIG DIPS IN SOME SEASONS",
subtitle = "Adjusted R-squared bounces around quite a bit in football\nfrom a range of 0.7 to 0.93"
) +
scale_y_continuous(limits = c(0.6, 1)) +
geom_smooth(data = subset(nfl_yearly_regressions, between(schedule_season, 1982, 2015)), method = "lm", colour = plot_cols[3], linetype = 6, se = F) +
geom_smooth(data = subset(nfl_yearly_regressions, between(schedule_season, 1966, 1981)), method = "lm", colour = plot_cols[4], linetype = 6, se = F) +
annotate("rect", xmin = 1982, xmax = 2015, ymin = -Inf, ymax = Inf, fill = plot_cols[3], alpha = 0.1) +
annotate("text", x = 1998, y = 0.95, label = "Then a long upward trend\nbetween 1982-2015", size = 5, colour = plot_cols[3]) +
annotate("rect", xmin = 1966, xmax = 1981, ymin = -Inf, ymax = Inf, fill = plot_cols[4], alpha = 0.1) +
annotate("text", x = 1973, y = 0.7, label = "This period saw the\nadjusted r-squared\ntrend downwards", size = 5, colour = plot_cols[4]) +
# annotate("rect", xmin=1996, xmax = 1981, ymin=-Inf, ymax = Inf, colour=plot_cols[3], alpha=0.1)
theme_jason()
styler:::style_selection()
nfl_expected_wins %>%
ggplot(aes(x = WinsAboveExpected)) +
geom_histogram(fill = plot_cols[2], colour = plot_cols[8], alpha = 0.5) +
labs(
title = "DIFFERENCES CENTERED AROUND ZERO",
subtitle = "Almost all team seasons have had wins above expected\nbetween -2.5 and 2.5 games"
) +
theme_jason() +
theme(
axis.title.y = element_blank(),
panel.grid.major.x = element_blank(), panel.grid.major.y = element_blank(),
panel.grid.minor.x = element_blank(), panel.grid.minor.y = element_blank()
)
styler:::style_selection()
nfl_expected_wins %>%
filter(abs(WinsAboveExpected) > 3) %>%
arrange(desc(WinsAboveExpected)) %>%
mutate(WinsAboveExpected = round(WinsAboveExpected, 2)) %>%
select(Season = schedule_season, Team, TotalWins, ExpectedWins, WinsAboveExpected) %>%
mutate(WinsAboveExpected = kableExtra::cell_spec(WinsAboveExpected, "html", color = ifelse(WinsAboveExpected < 0, plot_cols[3], plot_cols[2]))) %>%
kableExtra::kable(format = "html", escape = FALSE) %>%
kableExtra::kable_styling("striped", full_width = FALSE)
styler:::style_selection()
# create a df with performances against expectation
predicted_wins_nfl <- nfl_expected_wins %>%
mutate(ThisSeasonPerformance = ifelse(WinsAboveExpected < -1, "Under Performed",
ifelse(WinsAboveExpected > 1, "Over Performed", "Close to Expected")
)) %>%
arrange(Team, schedule_season) %>%
group_by(Team) %>%
mutate(
PreviousEWF = lag(ExpectedWinFactor),
PreviousWinsAboveExpected = lag(WinsAboveExpected),
PreviousSeasonPerformance = lag(ThisSeasonPerformance)
) %>%
filter(!is.na(PreviousEWF)) %>%
mutate(PredictedWins = PreviousEWF * GP) %>%
mutate(diff_predicted = TotalWins - PredictedWins)
styler:::style_selection()
?ggplot_build
styler:::style_selection()
p1 <- predicted_wins_nfl %>%
ggplot(aes(x = diff_predicted)) +
geom_density(fill = plot_cols[8], alpha = 0.1, colour = "lightgrey") +
#ggtitle("PREDICTING NEXT SEASONS WINS", subtitle = paste0("Using the previous year's Pythagorean Expectation win factor to predict wins for the current season\nresults in 50% of the errors being within ", round(quantile(predicted_wins_nfl$diff_predicted, probs = seq(0, 1, by = 0.05))[["25%"]], 2), " and ", round(quantile(predicted_wins_nfl$diff_predicted, probs = seq(0, 1, by = 0.05))[["75%"]], 2), " games off")) +
labs(
x = "Prediction Error",
title = "PREDICTING NEXT SEASONS WINS",
subtitle = paste0("Using the previous year's Pythagorean Expectation win factor to predict wins for the current season\nresults in 50% of the errors being within ", round(quantile(predicted_wins_nfl$diff_predicted, probs = seq(0, 1, by = 0.05))[["25%"]], 2), " and ", round(quantile(predicted_wins_nfl$diff_predicted, probs = seq(0, 1, by = 0.05))[["75%"]], 2), " games off")
) +
theme_jason() +
theme(
panel.grid.major.x = element_blank(), panel.grid.major.y = element_blank(),
panel.grid.minor.x = element_blank(), panel.grid.minor.y = element_blank()
)
b <- ggplot_build(p1)$data[[1]]
p1 + geom_area(
data = subset(b, between(
x, quantile(predicted_wins_nfl$diff_predicted)[["25%"]],
quantile(predicted_wins_nfl$diff_predicted)[["75%"]]
)),
aes(x = x, y = y),
fill = plot_cols[2], alpha = 0.5
) + # gives a nice border
annotate("text", x = -7.5, y = 0.075, label = "50% of our predictions fall\nwithin -1.89 and 2.06 games\nof actual wins", size = 5, colour = plot_cols[8]) +
geom_curve(
x = -5, y = 0.070, xend = 0, yend = 0.05,
arrow = arrow(length = unit(0.02, "npc")), curvature = 0.2, colour = plot_cols[8]
) +
scale_x_continuous(
breaks = c(-10, -5, -1.89, 0, 2.06, 5, 10),
labels = c("-10", "-5", "-1.89", "0", "2.06", "5", "10")
)
styler:::style_selection()
predicted_wins_nfl %>%
filter(abs(diff_predicted) > 8) %>%
arrange(desc(diff_predicted)) %>%
mutate(diff_predicted = round(diff_predicted, 2)) %>%
select(Season = schedule_season, Team, TotalWins, PredictedWins, PredictionError = diff_predicted) %>%
mutate(PredictionError = kableExtra::cell_spec(PredictionError, "html", color = ifelse(PredictionError < 0, plot_cols[3], plot_cols[2]))) %>%
kableExtra::kable(format = "html", escape = FALSE) %>%
kableExtra::kable_styling("striped", full_width = FALSE)
library(tidyverse)
library(espnscrapeR)
library(broom)
library(ggtext)
library(gt)
all_qbr <- 2006:2019 %>%
map_dfr(get_nfl_qbr)
all_wins <- 2006:2019 %>%
map_dfr(get_nfl_standings)
all_wins
all_qbr %>%
filter(season == 2019) %>%
mutate(above_50 = if_else())
group_by(name) %>%
mutate(
)
all_qbr %>% names()
all_qbr %>% View()
#name_display name_short
all_qbr %>%
filter(season == 2019) %>%
mutate(above_50 = if_else())
group_by(name_display) %>%
mutate(
)
all_qbr %>%
filter(season == 2019) %>%
mutate(above_50 = if_else())
all_qbr %>%
filter(season == 2019)
library(tidyverse)
library(tidymodels)
set.seed(37)
## ---- data-in
seasons <- 2016:2019
pbp_raw <- purrr::map_df(seasons, function(x) {
readRDS(
url(
glue::glue("~/Documents/nfl/data/pbp/play_by_play_{x}.rds")
)
)
})
set.seed(37)
## ---- data-in
seasons <- 2016:2019
pbp_raw <- purrr::map_df(seasons, function(x) {
readRDS(
glue::glue("~/Documents/nfl/data/pbp/play_by_play_{x}.rds")
)
})
styler:::style_selection()
season_df <- filter(
pbp_raw,
season_type == "REG" &
down %in% c(1, 2, 3) &
!is.na(qb_dropback) &
!is.na(score_differential)
) %>%
mutate(
qb_dropback = factor(qb_dropback),
off_to = if_else(posteam_type == "away", away_timeouts_remaining, home_timeouts_remaining),
def_to = if_else(posteam_type == "away", home_timeouts_remaining, away_timeouts_remaining)
) %>%
dplyr::select(qb_dropback, down, ydstogo, yardline_100, score_differential, qtr, half_seconds_remaining, off_to, def_to)
styler:::style_selection()
season_df1 <- pbp_raw %>%
select(
game_id, game_date, game_seconds_remaining, season_type, week, season,
play_type, yards_gained, ydstogo, down, yardline_100, qtr, posteam,
posteam_score, defteam, defteam_score, score_differential, shotgun,
no_huddle, posteam_timeouts_remaining, defteam_timeouts_remaining, penalty,
wp, goal_to_go, half_seconds_remaining
) %>%
filter(
play_type %in% c("run", "pass"),
penalty == 0,
season_type == "REG",
down %in% c(1:3),
!is.na(score_differential)
) %>%
mutate(
in_red_zone = if_else(yardline_100 <= 20, 1, 0),
in_fg_range = if_else(yardline_100 <= 35, 1, 0),
two_min_drill = if_else(half_seconds_remaining <= 120, 1, 0)
) %>%
select(-penalty, -season_type, -half_seconds_remaining)
season_df
season_df1
all_plays <- season_df %>%
group_by(game_id, posteam) %>%
mutate(
run = if_else(play_type == "run", 1, 0),
pass = if_else(play_type == "pass", 1, 0),
total_runs = if_else(play_type == "run", cumsum(run) - 1, cumsum(run)),
total_pass = if_else(play_type == "pass", cumsum(pass) - 1, cumsum(pass)),
previous_play = if_else(posteam == lag(posteam),
lag(play_type), "First play of Drive"
),
previous_play = if_else(is.na(previous_play),
replace_na("First play of Drive"), previous_play
)
) %>%
ungroup() %>%
mutate_at(vars(
play_type, season, posteam, defteam, shotgun, down, qtr, no_huddle,
posteam_timeouts_remaining, defteam_timeouts_remaining, in_red_zone,
in_fg_range, previous_play, goal_to_go, two_min_drill
), as.factor) %>%
select(-run, -pass)
all_plays <- season_df1 %>%
group_by(game_id, posteam) %>%
mutate(
run = if_else(play_type == "run", 1, 0),
pass = if_else(play_type == "pass", 1, 0),
total_runs = if_else(play_type == "run", cumsum(run) - 1, cumsum(run)),
total_pass = if_else(play_type == "pass", cumsum(pass) - 1, cumsum(pass)),
previous_play = if_else(posteam == lag(posteam),
lag(play_type), "First play of Drive"
),
previous_play = if_else(is.na(previous_play),
replace_na("First play of Drive"), previous_play
)
) %>%
ungroup() %>%
mutate_at(vars(
play_type, season, posteam, defteam, shotgun, down, qtr, no_huddle,
posteam_timeouts_remaining, defteam_timeouts_remaining, in_red_zone,
in_fg_range, previous_play, goal_to_go, two_min_drill
), as.factor) %>%
select(-run, -pass)
set.seed(37)
split_pbp <- initial_split(all_plays, 0.75, strata = play_type)
split_pbp
# separate the training data
train_data <- training(split_pbp)
# separate the testing data
test_data <- testing(split_pbp)
train_data %>%
count(play_type) %>%
mutate(ratio = n/sum(n))
test_data %>%
count(play_type) %>%
mutate(ratio = n/sum(n))
library(tidymodels)
pbp_rec <- recipe(play_type ~ ., data = train_data) %>%
# ignore these vars for train/test, but include in data as ID
update_role(game_id, game_date, yards_gained, new_role = "ID") %>%
# removes vars that have large absolute correlations w/ other vars
step_corr(all_numeric(), threshold = 0.7) %>%
step_center(all_numeric()) %>%  # substract mean from numeric
step_zv(all_predictors()) # remove zero-variance predictors
lr_mod <- logistic_reg(mode = "classification") %>%
set_engine("glm")
lr_wflow <- workflow() %>%
add_model(lr_mod) %>% # parsnip model
add_recipe(pbp_rec)
pbp_fit_lr <- lr_wflow %>%
fit(data = train_data)
pbp_pred_lr <- predict(pbp_fit_lr, test_data) %>%
# Get probabilities for the class for each observation
bind_cols(predict(pbp_fit_lr, test_data, type = "prob")) %>%
# Add back a "truth" column for what the actual play_type was
bind_cols(test_data %>% select(play_type, down, ydstogo, posteam))
pbp_pred_lr %>%
# get Area under Curve
roc_auc(truth = play_type,
.pred_pass)
pbp_pred_lr %>%
# collect and report metrics
metrics(truth = play_type,
.pred_class)
pbp_fit_lr
sum_fit <- pbp_pred_lr  %>%
filter(ydstogo <= 15) %>%
group_by(down, ydstogo) %>%
summarize(
.pred_pass = mean(.pred_pass),
n = n()
)
pbp_pred_lr  %>%
filter(between(ydstogo, 1, 15)) %>%
filter(posteam == "NE", down %in% c(1:3)) %>%
group_by(down, ydstogo) %>%
summarize(
.pred_pass = mean(.pred_pass),
n = n()
)
all_plays %>%
filter(between(ydstogo, 1, 15)) %>%
mutate(pass = if_else(play_type == "pass", 1, 0)) %>%
group_by(posteam, down, ydstogo) %>%
summarize(
n = n(),
pct_pass = mean(pass)
) %>%
mutate(posteam = as.character(posteam),
down = as.double(down)) %>%
ungroup() %>%
left_join(
pbp_pred_lr  %>%
filter(between(ydstogo, 1, 15)) %>%
mutate(posteam = as.character(posteam),
down = as.double(down)) %>%
filter(down %in% c(1:3)) %>%
group_by(posteam, down, ydstogo) %>%
summarize(
.pred_pass = mean(.pred_pass)
)
) %>%
ggplot(aes(x = pct_pass, y=.pred_pass)) +
geom_point()
all_plays %>%
filter(between(ydstogo, 1, 15)) %>%
mutate(pass = if_else(play_type == "pass", 1, 0)) %>%
group_by(down, ydstogo) %>%
summarize(
n = n(),
pct_pass = mean(pass)
) %>%
mutate(down = as.double(down)) %>%
ungroup() %>%
left_join(
pbp_pred_lr  %>%
filter(between(ydstogo, 1, 15)) %>%
mutate(posteam = as.character(posteam),
down = as.double(down)) %>%
filter(down %in% c(1:3)) %>%
group_by(down, ydstogo) %>%
summarize(
.pred_pass = mean(.pred_pass)
)
) %>%
ggplot(aes(x = pct_pass, y=.pred_pass)) +
geom_point()
pbp_pred_lr %>%
mutate(qb_dropback = if_else(play_type == "pass", 1, 0)) %>%
mutate(pred_rounded = round(.pred_pass,1)) %>%
group_by(pred_rounded) %>%
summarise(mean_prediction = mean(.pred_pass),
mean_actual = mean(as.numeric(qb_dropback)),
n = n()) %>%
ggplot(aes(x = pred_rounded, y = mean_actual)) +
geom_abline() +
geom_point(aes(size = n)) +
labs(
x = "Predicted Prob",
y = "Observed"
) +
scale_x_continuous(
breaks = seq(0, 1, by = 0.1), limits = c(0,1),
labels = scales::percent_format(accuracy = 1)
) +
scale_y_continuous(
breaks = seq(0, 1, by = 0.1), limits = c(0,1),
labels = scales::percent_format(accuracy = 1)
)
pbp_pred_lr %>%
filter(ydstogo <= 15) %>%
ggplot(aes(x = ydstogo, y = .pred_pass, color = down, group = down)) +
geom_point(data = sum_fit, aes(size = n)) +
geom_smooth() +
scale_y_continuous(
breaks = seq(0, 1, by = 0.1),
limits = c(0, 1),
labels = scales::percent_format(accuracy = 1)
) +
scale_x_continuous(breaks = seq(1, 15, by = 1)) +
geom_hline(yintercept = 0.5, color = "black", linetype = "dashed", alpha = 0.5) +
theme_minimal()
pbp_fit_lr
pbp_pred_lr %>%
filter(posteam == "NE")
styler:::style_selection()
rf_mod <- rand_forest(trees = 100) %>%
set_engine("ranger",
importance = "impurity", # variable importance
num.threads = 4
) %>% # Parallelize
set_mode("classification")
rf_wflow <- workflow() %>%
add_model(rf_mod) %>% # New model
add_recipe(pbp_rec) # Same recipe
pbp_fit_rf <- rf_wflow %>% # New workflow
fit(data = train_data) # Fit the Random Forest
# Get predictions and check metrics
pbp_pred_rf <- predict(pbp_fit_rf, test_data) %>%
bind_cols(test_data %>% select(play_type)) %>%
bind_cols(predict(pbp_fit_rf, test_data, type = "prob"))
pbp_pred_rf %>% # Random Forest predictions
metrics(truth = play_type, .pred_class)
pbp_pred_lr %>% # Logistic Regression predictions
metrics(truth = play_type, .pred_class)
pbp_fit_rf %>%
pull_workflow_fit() %>%
vip::vip(num_features = 20)
pbp_pred_rf %>%
mutate(qb_dropback = if_else(play_type == "pass", 1, 0)) %>%
mutate(pred_rounded = round(.pred_pass, 1)) %>%
group_by(pred_rounded) %>%
summarise(
mean_prediction = mean(.pred_pass),
mean_actual = mean(as.numeric(qb_dropback)),
n = n()
) %>%
ggplot(aes(x = pred_rounded, y = mean_actual)) +
geom_abline() +
geom_point(aes(size = n)) +
labs(
x = "Predicted Prob",
y = "Observed"
) +
scale_x_continuous(
breaks = seq(0, 1, by = 0.1), limits = c(0, 1),
labels = scales::percent_format(accuracy = 1)
) +
scale_y_continuous(
breaks = seq(0, 1, by = 0.1), limits = c(0, 1),
labels = scales::percent_format(accuracy = 1)
)
styler:::style_selection()
xg_mod <- boost_tree(trees = 100) %>%
set_engine("xgboost",
importance = "impurity", # variable importance
num.threads = 4
) %>% # Parallelize
set_mode("classification")
xg_wflow <- workflow() %>%
add_model(xg_mod) %>% # New model
add_recipe(pbp_rec) # Same recipe
pbp_fit_xg <- xg_wflow %>% # New workflow
fit(data = train_data) # Fit the Random Forest
xg_wflow <- workflow() %>%
add_model(xg_mod) %>% # New model
add_recipe(pbp_rec) # Same recipe
train_data
glimpse(train_data)
use_xgboost(play_type ~ ., data = train_data, prefix = "xgboost",
verbose = TRUE,
tune = TRUE,
colors = TRUE
)
library(usemodels)
use_xgboost(play_type ~ ., data = train_data, prefix = "xgboost",
verbose = TRUE,
tune = TRUE,
colors = TRUE
)
xgboost_recipe <- recipe(formula = play_type ~ ., data = train_data) %>%
step_string2factor(one_of(game_id, game_date)) %>%
step_novel(all_nominal(), -all_outcomes()) %>%
step_dummy(all_nominal(), -all_outcomes(), one_hot = TRUE) %>%
step_zv(all_predictors())
xgboost_spec <- boost_tree(trees = tune(), min_n = tune(), tree_depth = tune(), learn_rate = tune(), loss_reduction = tune(), sample_size = tune()) %>%
set_engine("xgboost", importance = "impurity",  num.threads = 4) %>%
set_mode("classification")
xgboost_workflow <- workflow() %>%
add_recipe(xgboost_recipe) %>%
add_model(xgboost_spec)
pbp_fit_xg <- xgboost_workflow %>% # New workflow
fit(data = train_data) # Fit the Random Forest
xgboost_workflow
set.seed(12429)
xgboost_tune <- tune_grid(xgboost_workflow, resamples = stop("add your rsample object"), grid = stop("add number of candidate points"))
tune_rf_wf
pbp_folds
post16 <- filter(pbp_raw,
season_type == 'REG' &
down %in% c(1,2,3) &
!is.na(qb_dropback) &
!is.na(score_differential),
qb_scramble == 0) %>%
mutate(qb_dropback = factor(qb_dropback),
off_to = if_else(posteam_type == 'away', away_timeouts_remaining, home_timeouts_remaining),
def_to = if_else(posteam_type == 'away', home_timeouts_remaining, away_timeouts_remaining)) %>%
dplyr::select(qb_dropback, down, ydstogo, yardline_100, score_differential, qtr, half_seconds_remaining, off_to, def_to)
dat_split <- initial_split(post16)
dat_train <- training(dat_split)
dat_test <- testing(dat_split)
qb_recipe <- recipe(qb_dropback ~ down +
ydstogo +
yardline_100 +
score_differential +
qtr +
half_seconds_remaining +
off_to +
def_to,
data = dat_train)
qb_model <- boost_tree(
mtry = tune(),
trees = 2000,
min_n = tune(),
tree_depth = tune(),
learn_rate = tune(),
loss_reduction = tune(),
sample_size = tune()
) %>%
set_engine("xgboost") %>%
set_mode("classification")
qb_folds <- vfold_cv(dat_train)
qb_workflow <- workflow() %>%
add_recipe(qb_recipe) %>%
add_model(qb_model)
xgb_grid <- grid_latin_hypercube(
finalize(mtry(), dat_train),
min_n(),
tree_depth(),
learn_rate(),
loss_reduction(),
sample_size = sample_prop(),
size = 30
)
xgb_res <- tune_grid(
qb_workflow,
resamples = qb_folds,
grid = xgb_grid,
control = control_grid(save_pred = TRUE)
)
