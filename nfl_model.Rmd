---
title: "R Notebook"
output: html_notebook
---

NFL Modelling

## Individual Expected Completion using Logistic Generalized Additive Mixed Models

To learn more about Logistic Mixed Effects I recommend https://stats.idre.ucla.edu/r/dae/mixed-effects-logistic-regression/

For a football case application of GAMs, nothing like M.Lopez’ post itself https://statsbylopez.netlify.app/post/plotting-air-yards/

### Packages and Data Preparation
```{r}
library(gamm4)
library(dplyr)
library(purrr)
library(ggplot2)
library(ggthemes)
library(scales)
```

```{r}
#GAMMs can take a while to run since they also perform cross-validation. So I’ll do my best to filter-out data as much as possible without affecting results. Hold on.

# Some stuff to filter later on
npass <- pbp %>%
  dplyr::filter(
    play_type == "pass",
    season_type == "REG"
  ) %>%
  mutate(
    play_in_19 = if_else(season == 2019, 1, 0)
  ) %>%
  group_by(passer_player_id) %>%
  dplyr::summarise(
    num_plays = n(),
    last_seas = max(season),
    plays_in_19 = sum(play_in_19)
  )

pbp2 <- merge(pbp, npass, by = "passer_player_id", all.x = T, no.dups = T)

# Mutations/data prep
pbp_mut <- pbp2 %>%
  dplyr::filter(
    season_type == "REG",
    wp <= .85,
    wp >= .15,
    play_type == "pass",
    !is.na(complete_pass),
    penalty == 0,
    num_plays >= 200
  ) %>%
  dplyr::mutate(
    ayard_is_zero = if_else(air_yards == 0, 1, 0),
    era1 = if_else(season %in% 2014:2017, 1, 0),
    away = if_else(home_team == posteam, 0, 1),
    id = passer_player_id,
    # fixing some weird bugs I found with names bugs don't affect model, but mess with plot
    passer_player_name = if_else(passer_player_name == "Jos.Allen", "J.Allen",
      if_else(passer_player_name == "R.Griffin", "R.Griffin III",
        if_else(passer_player_name == "Matt.Moore", "M.Moore",
          if_else(passer_player_name == "G.Minshew II", "G.Minshew", passer_player_name)
        )
      )
    )
  ) %>%
  dplyr::select(
    id, passer_player_name, era1, season, away, wind, temp, complete_pass,
    air_yards, qb_hit, ayard_is_zero, yardline_100, ydstogo, down,
    plays_in_19, yardline_100
  )

# To map id's and Quarterback names
names <- pbp_mut %>%
  group_by(id) %>%
  dplyr::summarise(
    Quarterback = unique(passer_player_name),
    last_seas = max(season),
    plays_in_19 = unique(plays_in_19)
  )
```

### Model

It would be a good idea to add non-linear components to ydstogo and yardline_100, but I don’t want to slow down the model too much. If you have the time to do it, go for it! just do s(ydstogo) and s(yardline_100).

Set family = binomial(link='logit') to make it a logistic binomial regression.
Our random effect will be id since we want to look at every QB intercept.
We do nACG = 0 to speed up the process, it technically sacrifices accuracy, but for this exercise is no big deal.
This will take around 5 minutes to run, depending on your computer.

```{r}
gam_model <- gamm4(
  complete_pass ~
  era1 +
    ydstogo +
    yardline_100 +
    down +
    away +
    qb_hit +
    ayard_is_zero +
    s(air_yards),
  random = ~ (1 | id),
  data = pbp_mut,
  nAGQ = 0,
  control = glmerControl(optimizer = "nloptwrap"),
  family = binomial(link = "logit")
)
```

### Retrieving Estimates and Prepare Data for Plot
```{r}
# Retreive estimates and standard errors
est <- broom.mixed::tidy(gam_model$mer, effects = "ran_vals") %>%
  dplyr::rename("id" = "level") %>%
  dplyr::filter(term == "(Intercept)")

# Function to convert logit to prob
logit2prob <- function(logit) {
  odds <- exp(logit)
  prob <- odds / (1 + odds)
  return(prob)
}

# Prepare data for plot
plot <- merge(est, names, by = "id", all.x = T, no.dups = T) %>%
  arrange(estimate) %>%
  mutate(
    lci = estimate - 1.96 * std.error,
    uci = estimate + 1.96 * std.error,
    prob = logit2prob(estimate),
    prob_uci = logit2prob(uci),
    prob_lci = logit2prob(lci),
  ) %>%
  dplyr::filter(
    plays_in_19 >= 100
  )

```

### Plottting
```{r}
#The first plot includes the intercept estimate as well as confidence intervals.

plot %>%
  filter(
    last_seas == 2019
  ) %>%
  ggplot(aes(x = factor(Quarterback, level = Quarterback), prob)) +
  geom_point(size = .7) +
  geom_linerange(size = .5, aes(
    ymin = prob_lci,
    ymax = prob_uci
  )) +
  coord_flip() +
  theme_bw() +
  labs(
    y = "iProbability of Completion",
    title = "Individual Probability of Completion per Quarterback",
    subtitle = "How each QB increases probability completion, controlling for situation | GAMM",
    caption = "Data: nflfastR | Analysis by Adrian Cadena @adrian_cadem"
  ) +
  theme(
    plot.title = element_text(size = 15, hjust = .5),
    plot.subtitle = element_text(size = 10, hjust = .5),
    axis.title.y = element_blank(),
  )
```

```{r}
#The second plot is a little easier to read and looks nice in terms of aesthetics. We are only adding the estimated probability intercept, no confidence interval.

plot %>%
  filter(last_seas == 2019) %>%
  ggplot(aes(x = factor(Quarterback, level = Quarterback), prob)) +
  geom_col(fill = "grey20") +
  geom_text(aes(label = Quarterback, y = (((prob - .43) * .5)) + .43), color = "white", hjust = 1, size = 2.6, vjust = 0.3) +
  coord_flip() +
  theme_bw() +
  labs(
    y = "iProbability of Completion",
    title = "Individual Probability of Completion per Quarterback",
    subtitle = "How each QB increases probability completion, controlling for situation | GAMM",
    caption = "Data: nflfastR | Analysis by Adrian Cadena @adrian_cadem"
  ) +
  scale_y_continuous(limits = c(.43, .565), oob = rescale_none, labels = scales::percent_format(accuracy = 1)) +
  theme(
    plot.title = element_text(size = 14, hjust = .5),
    plot.subtitle = element_text(size = 10, hjust = .5),
    axis.title.x = element_text(size = 12),
    axis.title.y = element_blank(),
    axis.text.y = element_blank(),
    axis.ticks.y = element_blank(),
  )
```

## Player Density and Completion Surface Estimates

https://www.opensourcefootball.com/posts/2020-08-29-player-density-and-completion-surface-estimates/

## Estimating Run/Pass Tendencies with tidyModels and nflfastR

## Estimating Team Ability From EPA

```{r}
library(rstan)
library(lme4)
library(tidyverse)
library(DT)
library(tidybayes)
library(ggrepel)
library(magick)
library(resample)

set.seed(1234)

seasons <- 2018:2020
dat <- purrr::map_df(seasons, function(x) {
  readRDS(
    url(
      glue::glue("https://raw.githubusercontent.com/guga31bb/nflfastR-data/master/data/play_by_play_{x}.rds")
    )
  )
})

post16 <- filter(dat,
                   season_type == 'REG' & 
                   !is.na(epa) &
                   play_type %in% c('pass','run') &
                   down %in% c(1,2,3,4)
                 ) %>%
  dplyr::select(season, week, posteam, defteam, epa, play_id, qtr, quarter_seconds_remaining) %>%
  mutate(def_id = as.numeric(factor(str_c(defteam, '-', season))),
         off_id = as.numeric(factor(str_c(posteam, '-', season))))

epa_off_actual <- post16 %>%
  group_by(posteam, season, off_id) %>%
  summarise(offensive_epa = mean(epa),
            n_offense = n())

epa_def_actual <- post16 %>%
  group_by(defteam, season, def_id) %>%
  summarise(defensive_epa = mean(epa))

epa_actual <- epa_off_actual %>%
  inner_join(epa_def_actual, by = c('posteam' = 'defteam','season','off_id' = 'def_id'))

chiefs_2019 <- filter(post16, week <= 4 & season == 2019 & (posteam == 'KC'|defteam == 'KC')) %>%
  arrange(week, play_id) %>%
  mutate(play_num = row_number(),
         bp = ifelse(epa < -13, 1, 0))

big_play_id <- which(chiefs_2019$bp == 1)

pre <- chiefs_2019 %>%
  filter(play_num < big_play_id,
         defteam == 'KC') %>%
  summarise(epa = mean(epa))

with <- chiefs_2019 %>%
  filter(play_num <= big_play_id,
         defteam == 'KC') %>%
  summarise(epa = mean(epa))

rm(dat)
```

### The Simplest Model

Let’s start by saying our goal is to get regularized estimates of offensive ability. The simplest multilevel model we could build is something like the equation below.

$$epa_{ij} ~ Normal(\alpha_{ij}, \sigma)$$

This says that the epa of play i when team j has possession is normally distributed with a mean of alphaij and standard deviation σ. We can think of alphaij as our estimate of team ability and to build the model we will put some hierarchical priors on this estimate. We’ll assume that EPA is centered at 0 and that team abilities follow a normal distribution with a standard deviation of σoff. More formally,

$$\alpha_{ij} ~ Normal(0, \sigma_{off})$$
The model is going to include all offensive plays from 2018 through 2020. This excludes penalties and two point conversions.

We can estimate this model with the stan code below.

```{stan output.var=}
data{
  int<lower = 0> N; //number of plays
  int<lower = 1> I; //number of teams
  int<lower = 0, upper = I> ii[N]; //indicator for offense
  real<lower = -16, upper = 10> y[N]; //epa
}
parameters{
  real<lower = 0> sigma_y; //error for normal distribution
  real<lower = 0> sigma_off; //standard deviation of offensive ability
  vector[I] alpha_off_raw;
}
transformed parameters{ //using non-centered paramaterization
  vector[I] alpha_off = alpha_off_raw * sigma_off;
}
model{
  //priors
  alpha_off_raw ~ normal(0,1);
  sigma_off ~ normal(.05,.03);
  sigma_y ~ normal(1,.2);
  
  //likelihood
    y ~ normal(alpha_off[ii], sigma_y);
}
```

```{r}
stanmod <- stan_model('stan-models/epa-per-play-offense-only.stan')
standat <- list(N = nrow(post16),
                I = length(unique(post16$off_id)),
                ii = post16$off_id,
                y = post16$epa
                )
fit_off <- sampling(stanmod, data = standat, cores = 4, chains = 4, iter = 2000)

```

```{r}
alpha_off <- rstan::extract(fit_off, pars = c('alpha_off'))$alpha_off
offense <- colMeans(alpha_off)
offense_se <- sqrt(colVars(alpha_off))
epa_actual$offense_only_estimate <- offense 
epa_actual$offense_only_se <- offense_se
epa_actual$offense_only_lower <- epa_actual$offense_only_estimate - epa_actual$offense_only_se
epa_actual$offense_only_upper <- epa_actual$offense_only_estimate + epa_actual$offense_only_se

filter(epa_actual, season == 2020) %>%
  ggplot(aes(y = reorder(posteam, offense_only_estimate), x = offensive_epa)) + 
  geom_point(shape = 1) +
  geom_point(aes(x = offense_only_estimate, y = posteam), colour = 'blue') +
  geom_linerange(aes(xmin = offense_only_lower, xmax = offense_only_upper)) +
  theme_minimal() +
  geom_vline(xintercept = 0, lty = 2) +
  labs(title = 'Estimated vs. Actual Offensive EPA/Play, 2020',
       subtitle = 'Model Estimate in Blue, +/- 1 s.e.') +
  ylab('Team') +
  xlab('Offensive EPA')
```

### Adding Defense

Building a model with offense and defense requires adding two additional parameters. We will use $\alpha_{ik}$ to represent defensive ability and we will need to estimate $\sigma_{def}$, the standard deviation in defensive ability. Our probability model is:

$$epa_{ijk} \sim Normal(\alpha_{ij} + \alpha_{ik}, \sigma)$$

$$\alpha_{ij} \sim Normal(0, \sigma_{off})$$

$$\alpha_{ik} \sim Normal(0, \sigma_{def})$$

```{r, eval = F}
data{
  int<lower = 0> N; //number of plays
  int<lower = 1> I; //number of teams
  int<lower = 0, upper = I> ii[N]; //indicator for offense
  int<lower = 0, upper = I> jj[N]; //indicator for defense
  real<lower = -16, upper = 10> y[N]; //epa
  int<lower = 1> N_rep; //number of samples for posterior density check
  int<lower = 1, upper = I> ii_rep[N_rep];
  int<lower = 1, upper = I> jj_rep[N_rep];
}
parameters{
  real<lower = 0> sigma_y; //error for t distribution
  real<lower = 0> sigma_off; //variance in offensive ability
  real<lower = 0> sigma_def; //variance in defensive ability
  vector[I] alpha_off_raw;
  vector[I] alpha_def_raw;
}
transformed parameters{ //using non-centered paramaterization
  vector[I] alpha_off = alpha_off_raw * sigma_off;
  vector[I] alpha_def = alpha_def_raw * sigma_def;
}
model{
  //priors
  alpha_off_raw ~ normal(0,1);
  alpha_def_raw ~ normal(0,1);
  sigma_off ~ normal(.06,.03);
  sigma_def ~ normal(.03,.03);
  sigma_y ~ normal(1,.2);
  
  //likelihood
    y ~ normal(alpha_off[ii] + alpha_def[jj], sigma_y);
}
generated quantities{
  vector[N_rep] y_rep;
  
  for (n in 1:N_rep){
    y_rep[n] = normal_rng(alpha_off[ii_rep[n]] + alpha_def[jj_rep[n]], sigma_y);
  }
}
```

We build the model in exactly the same way but add an indicator for the defense.

```{r}
stanmod_normal <- stan_model('stan-models/epa-per-play-normal.stan')
standat_normal <- list(N = nrow(post16),
                I = length(unique(post16$off_id)),
                ii = post16$off_id,
                jj = post16$def_id,
                y = post16$epa,
                N_rep = nrow(filter(post16, season == 2020)),
                ii_rep = filter(post16, season == 2020)$off_id,
                jj_rep = filter(post16, season == 2020)$def_id
                )
fit_normal <- sampling(stanmod_normal, data = standat_normal, cores = 4, chains = 4, iter = 2000)
```

Before jumping into team estimates we can look at our variance parameters. The model shows that there is likely more spread in offensive ability than defensive ability, though the two are fairly close.

```{r, echo = F}
offense_normal <- apply(rstan::extract(fit_normal, pars = c('alpha_off'))$alpha_off, 2, mean)
defense_normal <- apply(rstan::extract(fit_normal, pars = c('alpha_def'))$alpha_def, 2, mean)
epa_actual$offense_normal_estimate <- offense_normal
epa_actual$defense_normal_estimate <- defense_normal
alpha_off_normal <- rstan::extract(fit_normal, pars = c('alpha_off'))$alpha_off
alpha_def_normal <- rstan::extract(fit_normal, pars = c('alpha_def'))$alpha_def
offense_normal <- colMeans(alpha_off_normal)
offense_se_normal <- sqrt(colVars(alpha_off_normal))
defense_normal <- colMeans(alpha_def_normal)
defense_se_normal <- sqrt(colVars(alpha_def_normal))
epa_actual$offense_normal_estimate <- offense_normal
epa_actual$offense_normal_se <- offense_se_normal
epa_actual$offense_normal_lower <- epa_actual$offense_normal_estimate - epa_actual$offense_normal_se
epa_actual$offense_normal_upper <- epa_actual$offense_normal_estimate + epa_actual$offense_normal_se
epa_actual$defense_normal_estimate <- defense_normal
epa_actual$defense_normal_se <- defense_se_normal
epa_actual$defense_normal_lower <- epa_actual$defense_normal_estimate - epa_actual$defense_normal_se
epa_actual$defense_normal_upper <- epa_actual$defense_normal_estimate + epa_actual$defense_normal_se
print(fit_normal, pars = c('sigma_off','sigma_def','sigma_y'))
```

The plot below shows that this model is doing more than just pulling teams toward the middle which is good because it's supposed to be doing more!

```{r, echo = F}
filter(epa_actual, season == 2020) %>%
  ggplot(aes(y = reorder(posteam, offense_normal_estimate), x = offensive_epa)) + 
  geom_point(shape = 1) +
  geom_point(aes(x = offense_normal_estimate, y = posteam), colour = 'blue') +
  geom_linerange(aes(xmin = offense_normal_lower, xmax = offense_normal_upper)) +
  theme_minimal() +
  geom_vline(xintercept = 0, lty = 2) +
  labs(title = 'Estimated vs. Actual Offensive EPA/Play, 2020',
       subtitle = 'Model Estimate in Blue, +/- 1 s.e.') +
  ylab('Team') +
  xlab('Offensive EPA/Play')
```

We can do the same thing for defense, keeping in mind that negative is good for defense. The Rams look like the clear best defense with Detroit and Houston bringing up the rear.

```{r, echo = F}
filter(epa_actual, season == 2020) %>%
  ggplot(aes(y = reorder(posteam, desc(defense_normal_estimate)), x = defensive_epa)) + 
  geom_point(shape = 1) +
  geom_point(aes(x = defense_normal_estimate, y = posteam), colour = 'blue') +
  geom_linerange(aes(xmin = defense_normal_lower, xmax = defense_normal_upper)) +
  theme_minimal() +
  geom_vline(xintercept = 0, lty = 2) +
  labs(title = 'Estimated vs. Actual Defensive EPA/Play, 2020',
       subtitle = 'Model Estimate in Blue, +/- 1 s.e.') +
  ylab('Team') +
  xlab('Defensive EPA/Play')
```

The next plot shows how team offensive estimates compare across the two models. Teams above the line are helped by opponent adjustments and teams below the line are hurt by opponent adjustments. The first thing to note is that these differences aren't huge. While we want to adjust for opponent, doing so doesn't completely change our understanding of who is and isn't good. The team whose estimate improves the most is the Giants which makes sense given that they faced numbers 1, 2, 4(2x), 5, 6, 8, 10, 14(2x), 15, 20, 24(2x), 26, and 27 in the model's estimated defensive ability. The team whose estimate drops the most is the Colts which also makes sense as they share a division with the 31st, 30th, and 28th ranked defenses and also got to face the 32nd, 29th, 27th, 26th, and 25th ranked defenses. Still, the vast majority of teams change very little which is consistent with findings that adding adjustments for defenses faced doesn't do much to help predict out of sample.

```{r}
epa_actual %>%
    dplyr::left_join(
      nflfastR::teams_colors_logos %>% dplyr::select(team_abbr, team_logo_espn),
      by = c("posteam" = "team_abbr")
    ) %>%
    dplyr::mutate(
      grob = purrr::map(seq_along(team_logo_espn), function(x) {
        grid::rasterGrob(magick::image_read(team_logo_espn[[x]]))
      })
    ) %>%
  filter(season == 2020) %>%
  ggplot(aes(x = offense_only_estimate, y = offense_normal_estimate)) +
      ggpmisc::geom_grob(aes(x = offense_only_estimate, y = offense_normal_estimate, label = grob), vp.width = 0.05) +
    theme_minimal() +
  geom_smooth(method = 'lm', se = F) +
  xlab('Offense Only Model') +
  ylab('Offense + Defense Model') +
  labs(title = 'Estimated Offensive EPA/Play',
       subtitle = '')
```

One way to evaluate the fit of our model is to ask if the model could plausibly have generated the data that we observe. Above we discussed the generated quantities block of the stan model. In this block we can generate simulated data from our model's estimated parameters. If these simulated data sets look somewhat like what actually happened that's a good start to build confidence in our model. We set up our stan data to simulate every play of the 2020 season and we are hoping that the dark line (the actual EPA values) is consistent with the light blue lines (the EPA values simulated from our model).

```{r}
yrep <- rstan::extract(fit_normal, pars = 'y_rep')$y_rep[1:100,]
bayesplot::ppc_dens_overlay(post16$epa[post16$season == 2020], yrep) +
  labs(title = 'Posterior Predictive Check') +
  xlim(-16, 10)
```

Yikes! Our model is predicting too few plays near zero, too many in the -4 to -2 and 2 to 4 ranges, and too few on the tails. EPA is weirdly distributed. As we mentioned in the introduction there are a handful of extremely high leverage plays and clearly there are a lot of plays near zero. It also looks like there's a second mode which is probably due to this being a mixture of distributions with EPA on runs, EPA on incomplete passes, and EPA on completed passes each having their own district shape.

## Using the Student's T

Ultimately there isn't a "right" choice of distribution that will neatly produce what we see in the dark line above. What we're seeing is probably a mixture of distributions with EPA on runs, EPA on completed passes, and EPA on incompletions all having their own distributions. Still, there are things we can do that are going to fit the data better while being easy to implement. The Student's t distribution has more density near the mean and at the tails than the normal distribution which looks like what our model needs. Our new model is going to be:

$epa_{ijk} \sim StudentT(\nu, \alpha_{ij} + \alpha_{ik}, \sigma)$

$\nu = 6$

Our new parameter, $\nu$, is the degrees of freedom parameter which controls how fat the tails will be with lower values corresponding to more extreme values with more density on the mean. We set this to 6 which is small enough to allow for the kinds of values that we actually see in our data without putting significant density on values way above or below what we would expect to find.

The stan code to fit the model is basically the same as above but we add an entry to the data block for degrees of freedom and and change the likelihood from <code>normal</code> to <code>student_t</code>.

```{r, eval = F}
data{
  int<lower = 0> N; //number of plays
  int<lower = 1> I; //number of teams
  int<lower = 0, upper = I> ii[N]; //indicator for offense
  int<lower = 0, upper = I> jj[N]; //indicator for defense
  int df; //degrees of freedom for t distribution
  real<lower = -16, upper = 10> y[N]; //epa
  int<lower = 1> N_rep; //number of samples for posterior density check
  int<lower = 1, upper = I> ii_rep[N_rep];
  int<lower = 1, upper = I> jj_rep[N_rep];
}
parameters{
  real<lower = 0> sigma_y; //error for t distribution
  real<lower = 0> sigma_off; //variance in offensive ability
  real<lower = 0> sigma_def; //variance in defensive ability
  vector[I] alpha_off_raw;
  vector[I] alpha_def_raw;
}
transformed parameters{ //using non-centered paramaterization
  vector[I] alpha_off = alpha_off_raw * sigma_off;
  vector[I] alpha_def = alpha_def_raw * sigma_def;
}
model{
  //priors
  alpha_off_raw ~ normal(0,1);
  alpha_def_raw ~ normal(0,1);
  sigma_off ~ normal(.06,.03);
  sigma_def ~ normal(.03,.03);
  sigma_y ~ normal(1,.2);
  
  //likelihood
    y ~ student_t(df,alpha_off[ii] + alpha_def[jj], sigma_y);
}
generated quantities{
  vector[N_rep] y_rep;
  
  for (n in 1:N_rep){
    y_rep[n] = student_t_rng(df, alpha_off[ii_rep[n]] + alpha_def[jj_rep[n]], sigma_y);
  }
}
```

The only difference in the code to fit the model is that we add a <code>df</code> object to our data.

```{r}
stanmod_t <- stan_model('stan-models/epa-per-play.stan')
standat_t <- list(N = nrow(post16),
                I = length(unique(post16$off_id)),
                ii = post16$off_id,
                jj = post16$def_id,
                df = 6,
                y = post16$epa,
                N_rep = nrow(filter(post16, season == 2020)),
                ii_rep = filter(post16, season == 2020)$off_id,
                jj_rep = filter(post16, season == 2020)$def_id
                )
fit_t <- sampling(stanmod_t, data = standat_t, cores = 4, chains = 4, iter = 2000)
```

Our estimates of the variance in team abilities are both slightly lower but very similar to what we saw in the normal model. They $\sigma_{y}$ parameter is much smaller but being a different distribution it's not a 1 to 1 comparison with the same parameter in the normal model. 

```{r, echo = F}
offense_t <- apply(rstan::extract(fit_t, pars = c('alpha_off'))$alpha_off, 2, mean)
defense_t <- apply(rstan::extract(fit_t, pars = c('alpha_def'))$alpha_def, 2, mean)
epa_actual$offense_t_estimate <- offense_t
epa_actual$defense_t_estimate <- defense_t
alpha_off_t <- rstan::extract(fit_t, pars = c('alpha_off'))$alpha_off
alpha_def_t <- rstan::extract(fit_t, pars = c('alpha_def'))$alpha_def
offense_t <- colMeans(alpha_off_t)
offense_se_t <- sqrt(colVars(alpha_off_t))
defense_t <- colMeans(alpha_def_t)
defense_se_t <- sqrt(colVars(alpha_def_t))
epa_actual$offense_t_estimate <- offense_t
epa_actual$offense_t_se <- offense_se_t
epa_actual$offense_t_lower <- epa_actual$offense_t_estimate - epa_actual$offense_t_se
epa_actual$offense_t_upper <- epa_actual$offense_t_estimate + epa_actual$offense_t_se
epa_actual$defense_t_estimate <- defense_t
epa_actual$defense_t_se <- defense_se_t
epa_actual$defense_t_lower <- epa_actual$defense_t_estimate - epa_actual$defense_t_se
epa_actual$defense_t_upper <- epa_actual$defense_t_estimate + epa_actual$defense_t_se
print(fit_t, pars = c('sigma_off','sigma_def','sigma_y'))
```

So is this a better model? Our posterior predictive check certainly looks better. We're capturing a lot more of the density in the center of the distribution and doing a better job with the tails. It's still not good but this model is clearly a more plausible candidate to have generated our data than the normal model.

```{r, echo = F}
yrep <- rstan::extract(fit_t, pars = 'y_rep')$y_rep[1:100,]
bayesplot::ppc_dens_overlay(post16$epa[post16$season == 2020], yrep) +
  labs(title = "Posterior Predictive Check, Student's t Model") +
  xlim(-16, 10)
```

Here you can see the offensive team ability estimates and now we have some interesting things happening! Buffalo and Kansas City both jump over Green Bay despite Green Bay having a much higher EPA/Play.

```{r, echo = F}
filter(epa_actual, season == 2020) %>%
  ggplot(aes(y = reorder(posteam, offense_t_estimate), x = offensive_epa)) + 
  geom_point(shape = 1) +
  geom_point(aes(x = offense_t_estimate, y = posteam), colour = 'blue') +
  geom_linerange(aes(xmin = offense_t_lower, xmax = offense_t_upper)) +
  theme_minimal() +
  geom_vline(xintercept = 0, lty = 2) +
  labs(title = 'Estimated vs. Actual Offensive EPA/Play, 2020',
       subtitle = 'Model Estimate in Blue, +/- 1 s.e.') +
  ylab('Team') +
  xlab('Offensive EPA/Play')
```

Detroit remains the worst defense but Pittsburgh passes the Rams to become the best defense in the league.

```{r, echo = F}
filter(epa_actual, season == 2020) %>%
  ggplot(aes(y = reorder(posteam, defense_t_estimate), x = defensive_epa)) + 
  geom_point(shape = 1) +
  geom_point(aes(x = defense_t_estimate, y = posteam), colour = 'blue') +
  geom_linerange(aes(xmin = defense_t_lower, xmax = defense_t_upper)) +
  theme_minimal() +
  geom_vline(xintercept = 0, lty = 2) +
  labs(title = 'Estimated vs. Actual Defensive EPA/Play, 2020',
       subtitle = 'Model Estimate in Blue, +/- 1 s.e.') +
  ylab('Team') +
  xlab('Defensive EPA/Play')
```

Below we can see how estimates compare across the normal and Student's t models. Generally speaking the estimates are lower, but more importantly we see some big differences in our team estimates. The Bills, Vikings, Patriots, Rams, and Cowboys look a good deal better while the Packers, Steelers, and Chargers look worse.

```{r}
epa_actual %>%
    dplyr::left_join(
      nflfastR::teams_colors_logos %>% dplyr::select(team_abbr, team_logo_espn),
      by = c("posteam" = "team_abbr")
    ) %>%
    dplyr::mutate(
      grob = purrr::map(seq_along(team_logo_espn), function(x) {
        grid::rasterGrob(magick::image_read(team_logo_espn[[x]]))
      })
    ) %>%
  filter(season == 2020) %>%
  ggplot(aes(x = offense_normal_estimate, y = offense_t_estimate)) +
      ggpmisc::geom_grob(aes(x = offense_normal_estimate, y = offense_t_estimate, label = grob), vp.width = 0.05) +
    theme_minimal() +
  geom_smooth(method = 'lm', se = F) +
  xlab('Normal Model') +
  ylab("Student's t Model") +
  labs(title = 'Estimated Offensive EPA/Play',
       subtitle = '')
```

The same plot for defense shows some big moves. The Steelers, 49ers, Bears, and Eagles improve while the Colts, Dolphins, Bills, and Patriots drop.

```{r, echo = F}
epa_actual %>%
    dplyr::left_join(
      nflfastR::teams_colors_logos %>% dplyr::select(team_abbr, team_logo_espn),
      by = c("posteam" = "team_abbr")
    ) %>%
    dplyr::mutate(
      grob = purrr::map(seq_along(team_logo_espn), function(x) {
        grid::rasterGrob(magick::image_read(team_logo_espn[[x]]))
      })
    ) %>%
  filter(season == 2020) %>%
  ggplot(aes(x = defense_normal_estimate, y = defense_t_estimate)) +
      ggpmisc::geom_grob(aes(x = defense_normal_estimate, y = defense_t_estimate, label = grob), vp.width = 0.05) +
    theme_minimal() +
  geom_smooth(method = 'lm', se = F) +
  xlab('Normal Model') +
  ylab("Student's t Model") +
  labs(title = 'Estimated Defensive EPA/Play',
       subtitle = '')
```

The obvious question is why certain teams are moving so much. What makes Miami so much worse and Pittsburgh so much better when using the Student's t? The Student's t puts more density on extreme values which makes our mean estimates more robust to outliers. This results in teams that get to a good EPA/Play by racking up a few huge plays looking worse than if we use the normal. Likewise, teams whose EPA/Play are dragged down by a few big plays are going to look better. Going back to the example at the start of the paper, this approach would give the Chiefs a lot less credit for that big play than if we use the normal distribution.

Another way to look at how this model is arriving at its team strength estimates is to compare Miami and San Francisco who are very close in the normal model and diverge sharply with the Student's t. The plot below shows the density of defensive EPA for each team. Miami has a good number of plays on the far left of the distribution, which makes sense given all of the pick 6's they had in the early to middle part of the season, but San Francisco has had better results on plays near the center of the distribution. 

```{r}
filter(post16, season == 2020 & defteam %in% c('MIA','SF')) %>%
  ggplot(aes(x = epa, colour = defteam)) + 
  scale_colour_manual(values = c('#008e97','#aa0000')) +
  geom_density() +
  theme_minimal() +
  xlab("EPA") +
  ylab("") +
  labs(title = "Defensive EPA, Miami and San Francisco",
       colour = 'Team')
```

Pittsburgh and the Rams tell a similar story. Pittsburgh allowed more big plays than the Rams and the Rams forced some very high leverage turnovers but Pittsburgh was better near the center of the distribution.

```{r, echo = F}
filter(post16, season == 2020 & defteam %in% c('PIT','LA')) %>%
  ggplot(aes(x = epa, colour = defteam)) + 
  scale_colour_manual(values = c('#002244','#ffb612')) +
  geom_density() +
  theme_minimal() +
  xlab("EPA") +
  ylab("") +
  labs(title = "Defensive EPA, Pittsburgh and Los Angeles Rams",
       colour = 'Team')
```

We can do the same for New England and the Steelers on offense. New England has performed better on plays near the bulk of the distribution while having more disastrous plays and fewer big positive plays which I would guess lines up with the experience of most Pats fans. 

```{r, echo = F}
filter(post16, season == 2020 & posteam %in% c('NE','PIT')) %>%
  ggplot(aes(x = epa, colour = posteam)) + 
  scale_colour_manual(values = c("#002244","#ffb612")) +
  geom_density() +
  theme_minimal() +
  xlab("EPA") +
  ylab("") +
  labs(title = "Offensive EPA, New England and Pittsburgh",
       colour = 'Team')
```

## Conclusions

The goal of this post was to introduce how you can use publicly available information to build a model that estimates team abilities while incorporating information about quality of competition. In so doing we found that EPA/Play is an extremely noisy measure that requires a good deal of regularization. Big plays and differences in competition can greatly affect how we view teams so we need to make sure that these are accounted for. This has important implications for what kinds of conclusions we can draw from splits in the data. You can look at EPA/Play against tight ends or EPA/Play weeks 1-3 vs. weeks 4-6 but you should probably shouldn't read too much into them. When you're looking at EPA you need to bet heavily on regression to the mean.

All that said, this model needs a lot of work! This was a very simple model and there are all kinds of extensions that you could build into this framework to make the model better. You might want to build in information about the quarterback for a given play to account for situations like the Cowboys where you have a very good quarterback for 4.5 games and some less good quarterbacks for the remaining games (just make sure you have a model to estimate QB ability with the appropriate amount of uncertainty!). You could model team abilities as coming from a different distribution. You could build in an autoregressive structure to allow team ability to vary over time or to take advantage of the fact that we can probably learn something about the 2020 Chiefs from the 2019 Chiefs. You could include information about win probability or coaching staffs or weather or betting lines or whatever you can imagine to take advantage of all of the information that you think is important. This post provides a framework and hopefully the community will find it useful and can build from it.

## Appendix: Team Estimates

```{r}
epa_actual %>%
  dplyr::select(posteam, season, offense_normal_estimate, offense_t_estimate, defense_normal_estimate, defense_t_estimate) %>%
  mutate_if(is.numeric, round, 2) %>%
  arrange(desc(offense_t_estimate)) %>%
  rename('Team' = posteam,
         'Season' = season,
         'Offense, Normal' = offense_normal_estimate,
         'Offense, T' = offense_t_estimate,
         'Defense, Normal' = defense_normal_estimate,
         'Defense, T' = defense_t_estimate) %>%
  datatable(filter = 'top')
```



